import pandas as pd
import json
from datetime import datetime, timedelta
import time
from typing import Dict, List, Any
import os
import glob
from concurrent.futures import ThreadPoolExecutor
import numpy as np
import hashlib

from config.settings import settings
from config.models import AgentResponse
from tools.apify_client import ApifyDataCollector
from tools.llm_client import MultiProviderLLM
from tools.cache_manager import CacheManager

class AdvancedResearchAgent:
    def __init__(self):
        self.llm_client = MultiProviderLLM()
        self.apify_client = ApifyDataCollector()
        self.cache_manager = CacheManager()
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.data_retention_days = 1
    
    def _generate_cache_key(self, business_type: str, location: str, research_type: str) -> str:
        """Generate unique cache key based on business type AND location"""
        # Normalize inputs to avoid case sensitivity issues
        normalized_business = business_type.lower().strip().replace(' ', '_')
        normalized_location = location.lower().strip().replace(' ', '_').replace(',', '_')
        
        key_string = f"{normalized_business}_{normalized_location}_{research_type}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _should_use_cached_data(self, business_type: str, location: str) -> bool:
        """Check if we should use cached data for this specific business type and location"""
        cache_key = self._generate_cache_key(business_type, location, "advanced")
        cached = self.cache_manager.get_cached_result(cache_key)
        
        if not cached:
            return False
        
        # Additional validation: check if cached data matches our current request
        try:
            cached_data = cached.get('data', {})
            cached_metadata = cached_data.get('metadata', {})
            cached_business = cached_metadata.get('business_type', '').lower()
            cached_location = cached_metadata.get('location', '').lower()
            
            current_business = business_type.lower()
            current_location = location.lower()
            
            # Only use cache if business type AND location match exactly
            if (cached_business == current_business and 
                cached_location == current_location):
                print(f"ðŸŽ¯ Valid cache found for {business_type} in {location}")
                return True
            else:
                print(f"ðŸ”„ Cache mismatch: {cached_business}/{cached_location} vs {current_business}/{current_location}")
                return False
                
        except Exception as e:
            print(f"âš ï¸ Cache validation error: {e}")
            return False
    
    def conduct_research(self, business_type: str, location: str, use_cache: bool = True) -> AgentResponse:
        """Conduct comprehensive market research with proper location-specific caching"""
        start_time = time.time()
        print(f"ðŸš€ Starting advanced research: {business_type} in {location}")
        
        # Check cache for previous research results with proper validation
        if use_cache and self._should_use_cached_data(business_type, location):
            cache_key = self._generate_cache_key(business_type, location, "advanced")
            cached = self.cache_manager.get_cached_result(cache_key)
            if cached:
                print(f"ðŸŽ¯ Using cached research results for {business_type} in {location}")
                return AgentResponse(**cached['data'])
        
        try:
            # Step 1: Always collect fresh data for new locations
            print(f"ðŸŒ Collecting FRESH market data for {location}...")
            market_data = self._collect_fresh_market_data(business_type, location)
            
            # Step 2: Comprehensive analysis
            print("ðŸ“Š Analyzing market dynamics...")
            analysis_results = self._perform_comprehensive_analysis(market_data, business_type, location)
            
            # Step 3: Generate visualization data
            print("ðŸ“ˆ Preparing visualization data...")
            visualization_data = self._prepare_visualization_data(market_data, analysis_results)
            
            # Step 4: Generate business intelligence
            print("ðŸ’¼ Generating business intelligence...")
            business_report = self._generate_business_intelligence(analysis_results, business_type, location)
            
            execution_time = time.time() - start_time
            
            response_data = {
                "reasoning": business_report["executive_summary"],
                "data": {
                    "market_analysis": analysis_results,
                    "business_intelligence": business_report,
                    "visualization_data": visualization_data,
                    "raw_data_summary": {
                        "total_businesses": len(market_data.get("places_data", [])),
                        "total_trends": len(market_data.get("trends_data", [])),
                        "data_freshness": market_data.get("metadata", {}).get("data_freshness", "fresh"),
                        "location": location,
                        "business_type": business_type
                    }
                },
                "insights": business_report["strategic_insights"],
                "confidence": business_report["confidence_score"],
                "metadata": {
                    "execution_time": round(execution_time, 2),
                    "llm_provider": self.llm_client.active_provider,
                    "data_source": market_data.get("metadata", {}).get("data_source", "fresh"),
                    "timestamp": datetime.now().isoformat(),
                    "business_type": business_type,
                    "location": location,
                    "cache_key": self._generate_cache_key(business_type, location, "advanced")
                }
            }
            
            # Cache results with proper key
            if use_cache:
                cache_key = self._generate_cache_key(business_type, location, "advanced")
                self.cache_manager.save_result_to_cache(cache_key, response_data)
                print(f"ðŸ’¾ Cached fresh data for {business_type} in {location}")
            
            print(f"âœ… Research completed in {execution_time:.2f}s for {location}")
            return AgentResponse(**response_data)
            
        except Exception as e:
            print(f"âŒ Research failed for {location}: {e}")
            return self._create_fallback_response(business_type, location)
    
    # In research_agent.py, replace the data collection methods:

    def _collect_fresh_market_data(self, business_type: str, location: str) -> Dict[str, Any]:
        """Collect fresh market data using SearchAPI"""
        print(f"ðŸ” Collecting FRESH data using SearchAPI for {business_type} in {location}...")
        
        places_data = []
        trends_data = []
        related_searches = []
        
        try:
            # Collect Google Maps data for competitors
            print(f"ðŸ¢ Searching Google Maps for competitors in {location}...")
            search_queries = [
                f"{business_type} in {location}",
                f"best {business_type} {location}",
                f"{business_type} services {location}"
            ]
            
            for query in search_queries:
                places_data.extend(self.searchapi_client.search_google_maps(query, location))
            
            # Collect Google Trends data for market analysis
            print(f"ðŸ“ˆ Getting Google Trends for {business_type} in {location}...")
            trends_keywords = [
                business_type,
                f"{business_type} services",
                f"{business_type} {location}"
            ]
            
            for keyword in trends_keywords:
                trends_data.extend(self.searchapi_client.search_google_trends(keyword, "IN"))
            
            # Get related searches for opportunity analysis
            print(f"ðŸ” Getting related searches for {business_type}...")
            related_searches = self.searchapi_client.get_related_searches(business_type)
            
            # Save data with location-specific filename
            if places_data:
                self._save_data_with_timestamp(places_data, business_type, location, "places")
            if trends_data:
                self._save_data_with_timestamp(trends_data, business_type, location, "trends")
            
            print(f"âœ… Collected {len(places_data)} places, {len(trends_data)} trends, and {len(related_searches)} related searches for {location}")
                
        except Exception as e:
            print(f"âš ï¸ Data collection issue for {location}: {e}")
        
        return {
            "places_data": places_data,
            "trends_data": trends_data,
            "related_searches": related_searches,
            "metadata": {
                "business_type": business_type,
                "location": location,
                "collection_time": datetime.now().isoformat(),
                "data_source": "searchapi_google",
                "data_freshness": "very_fresh"
            }
        }
    
    def _collect_smart_market_data(self, business_type: str, location: str) -> Dict[str, Any]:
        """Smart data collection that ensures location-specific data"""
        print(f"ðŸ” Collecting fresh data for {location}...")
        
        # Always collect fresh data for new locations
        places_data = []
        trends_data = []
        
        try:
            # Collect fresh places data for the specific location
            print(f"ðŸŒ Collecting fresh places data for {location}...")
            places_data = self.apify_client.scrape_places_data([
                f"{business_type} in {location}", 
                f"best {business_type} {location}",
                f"{business_type} {location}"
            ])
            
            # Collect fresh trends data for the specific location
            print(f"ðŸŒ Collecting fresh trends data for {location}...")
            trends_data = self.apify_client.scrape_trends_data(
                [business_type, f"{business_type} services", f"{business_type} {location}"], 
                location
            )
            
            # Save with location-specific filename
            if places_data:
                self._save_data_with_timestamp(places_data, business_type, location, "places")
            if trends_data:
                self._save_data_with_timestamp(trends_data, business_type, location, "trends")
                
        except Exception as e:
            print(f"âš ï¸ Data collection issue for {location}: {e}")
        
        return {
            "places_data": places_data,
            "trends_data": trends_data,
            "metadata": {
                "business_type": business_type,
                "location": location,
                "collection_time": datetime.now().isoformat(),
                "data_source": "fresh",
                "data_freshness": "very_fresh"
            }
        }
    
    def _perform_comprehensive_analysis(self, market_data: Dict, business_type: str, location: str) -> Dict[str, Any]:
        """Perform comprehensive market analysis"""
        analysis = {}
        
        # 1. Competitive Analysis with real business data
        if market_data.get("places_data"):
            print("ðŸ¢ Analyzing competitive landscape...")
            analysis["competitive_analysis"] = self._analyze_real_competition(
                market_data["places_data"], business_type, location
            )
        
        # 2. Market Trends Analysis
        if market_data.get("trends_data"):
            print("ðŸ“ˆ Analyzing market trends...")
            analysis["trends_analysis"] = self._analyze_real_trends(
                market_data["trends_data"], business_type, location
            )
        
        # 3. Locality Analysis
        analysis["locality_analysis"] = self._analyze_locality_dynamics(analysis, business_type, location)
        
        # 4. Enhanced EDA Analysis
        analysis["eda_analysis"] = self._perform_enhanced_eda(market_data, business_type, location)
        
        return analysis
    
    def _perform_enhanced_eda(self, market_data: Dict, business_type: str, location: str) -> Dict[str, Any]:
        """Perform enhanced exploratory data analysis"""
        eda_results = {
            "data_quality": {},
            "market_patterns": {},
            "risk_indicators": {},
            "opportunity_metrics": {}
        }
        
        # Analyze data quality
        if market_data.get("places_data"):
            places_df = pd.DataFrame(market_data["places_data"])
            eda_results["data_quality"]["places"] = self._assess_places_data_quality(places_df)
        
        if market_data.get("trends_data"):
            trends_df = pd.DataFrame(market_data["trends_data"])
            eda_results["data_quality"]["trends"] = self._assess_trends_data_quality(trends_df)
        
        # Extract market patterns
        eda_results["market_patterns"] = self._extract_market_patterns(market_data)
        
        # Calculate risk and opportunity metrics
        eda_results["risk_indicators"] = self._calculate_risk_indicators(market_data)
        eda_results["opportunity_metrics"] = self._calculate_opportunity_metrics(market_data)
        
        return eda_results
    
    def _assess_places_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Assess quality of places data"""
        quality_metrics = {
            "completeness_score": 0,
            "reliability_score": 0,
            "coverage_score": 0,
            "issues": []
        }
        
        # Check for critical columns
        critical_columns = ['name', 'rating', 'user_ratings_total']
        missing_columns = [col for col in critical_columns if col not in df.columns]
        
        if missing_columns:
            quality_metrics["issues"].append(f"Missing critical columns: {missing_columns}")
        
        # Calculate completeness
        total_cells = df.size
        missing_cells = df.isnull().sum().sum()
        completeness = (total_cells - missing_cells) / total_cells if total_cells > 0 else 0
        quality_metrics["completeness_score"] = round(completeness, 2)
        
        # Assess reliability through rating distribution
        if 'rating' in df.columns:
            rating_stats = df['rating'].describe()
            if rating_stats['std'] < 0.5:
                quality_metrics["issues"].append("Rating distribution may be biased")
            quality_metrics["reliability_score"] = 0.8  # Base reliability
        
        # Coverage assessment
        quality_metrics["coverage_score"] = min(len(df) / 50, 1.0)  # Normalize to 50 businesses
        
        return quality_metrics
    
    def _assess_trends_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Assess quality of trends data"""
        quality_metrics = {
            "temporal_coverage": 0,
            "consistency_score": 0,
            "volatility_analysis": "unknown",
            "issues": []
        }
        
        if 'date' in df.columns and 'value' in df.columns:
            # Check temporal coverage
            dates = pd.to_datetime(df['date'])
            date_range = (dates.max() - dates.min()).days
            quality_metrics["temporal_coverage"] = min(date_range / 90, 1.0)  # Normalize to 90 days
            
            # Check consistency
            value_std = df['value'].std()
            quality_metrics["consistency_score"] = 1.0 - min(value_std / 50, 1.0)  # Normalize volatility
            
            # Volatility analysis
            if value_std > 30:
                quality_metrics["volatility_analysis"] = "high"
            elif value_std > 15:
                quality_metrics["volatility_analysis"] = "medium"
            else:
                quality_metrics["volatility_analysis"] = "low"
        
        return quality_metrics
    
    def _extract_market_patterns(self, market_data: Dict) -> Dict[str, Any]:
        """Extract meaningful market patterns from data"""
        patterns = {
            "customer_preferences": {},
            "competitive_dynamics": {},
            "market_maturity": "unknown",
            "seasonality_patterns": "unknown"
        }
        
        if market_data.get("places_data"):
            places_df = pd.DataFrame(market_data["places_data"])
            
            # Analyze customer preferences through ratings and reviews
            if 'rating' in places_df.columns and 'user_ratings_total' in places_df.columns:
                patterns["customer_preferences"] = {
                    "preferred_rating_range": self._get_preferred_rating_range(places_df),
                    "review_sentiment": self._analyze_review_sentiment(places_df)
                }
            
            # Competitive dynamics
            patterns["competitive_dynamics"] = self._analyze_competitive_dynamics(places_df)
            
            # Market maturity assessment
            patterns["market_maturity"] = self._assess_market_maturity(places_df)
        
        if market_data.get("trends_data"):
            trends_df = pd.DataFrame(market_data["trends_data"])
            patterns["seasonality_patterns"] = self._detect_seasonality(trends_df)
        
        return patterns
    
    def _get_preferred_rating_range(self, df: pd.DataFrame) -> str:
        """Determine preferred rating range in the market"""
        if 'rating' not in df.columns:
            return "unknown"
        
        rating_avg = df['rating'].mean()
        if rating_avg >= 4.5:
            return "excellent_quality"
        elif rating_avg >= 4.0:
            return "high_quality" 
        elif rating_avg >= 3.5:
            return "good_quality"
        else:
            return "average_quality"
    
    def _analyze_review_sentiment(self, df: pd.DataFrame) -> str:
        """Analyze review sentiment based on volume and ratings"""
        if 'user_ratings_total' not in df.columns or 'rating' not in df.columns:
            return "unknown"
        
        total_reviews = df['user_ratings_total'].sum()
        avg_rating = df['rating'].mean()
        
        if total_reviews > 1000 and avg_rating >= 4.0:
            return "highly_positive"
        elif total_reviews > 500 and avg_rating >= 3.5:
            return "positive"
        elif total_reviews > 100:
            return "moderate"
        else:
            return "limited_data"
    
    def _analyze_competitive_dynamics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze competitive dynamics in the market"""
        dynamics = {
            "price_competition": "unknown",
            "quality_competition": "unknown",
            "market_concentration": "unknown"
        }
        
        # Price competition analysis
        if 'priceLevel' in df.columns:
            price_std = df['priceLevel'].std()
            if price_std > 1.0:
                dynamics["price_competition"] = "diverse_pricing"
            else:
                dynamics["price_competition"] = "similar_pricing"
        
        # Quality competition
        if 'rating' in df.columns:
            rating_std = df['rating'].std()
            if rating_std > 0.8:
                dynamics["quality_competition"] = "varied_quality"
            elif rating_std > 0.4:
                dynamics["quality_competition"] = "moderate_quality_range"
            else:
                dynamics["quality_competition"] = "consistent_quality"
        
        # Market concentration (Herfindahl-like index)
        if 'name' in df.columns:
            total_businesses = len(df)
            if total_businesses > 0:
                # Simple concentration measure
                top_5_reviews = df.nlargest(5, 'user_ratings_total')['user_ratings_total'].sum() if 'user_ratings_total' in df.columns else 0
                total_reviews = df['user_ratings_total'].sum() if 'user_ratings_total' in df.columns else 1
                concentration = top_5_reviews / total_reviews
                
                if concentration > 0.7:
                    dynamics["market_concentration"] = "highly_concentrated"
                elif concentration > 0.5:
                    dynamics["market_concentration"] = "moderately_concentrated"
                else:
                    dynamics["market_concentration"] = "fragmented"
        
        return dynamics
    
    def _assess_market_maturity(self, df: pd.DataFrame) -> str:
        """Assess market maturity based on business characteristics"""
        total_businesses = len(df)
        
        if total_businesses > 30:
            return "mature"
        elif total_businesses > 15:
            return "developing"
        elif total_businesses > 5:
            return "emerging"
        else:
            return "nascent"
    
    def _detect_seasonality(self, trends_df: pd.DataFrame) -> str:
        """Detect seasonal patterns in trends data"""
        if 'date' not in trends_df.columns or 'value' not in trends_df.columns:
            return "unknown"
        
        try:
            df = trends_df.copy()
            df['date'] = pd.to_datetime(df['date'])
            df['month'] = df['date'].dt.month
            
            monthly_avg = df.groupby('month')['value'].mean()
            if len(monthly_avg) >= 6:
                variation = monthly_avg.std() / monthly_avg.mean()
                if variation > 0.4:
                    return "strong_seasonality"
                elif variation > 0.2:
                    return "moderate_seasonality"
                else:
                    return "low_seasonality"
        except:
            pass
        
        return "unknown"
    
    def _calculate_risk_indicators(self, market_data: Dict) -> Dict[str, Any]:
        """Calculate risk indicators for the market"""
        risks = {
            "competition_risk": "medium",
            "demand_risk": "medium", 
            "data_reliability_risk": "low",
            "market_volatility_risk": "medium"
        }
        
        if market_data.get("places_data"):
            places_df = pd.DataFrame(market_data["places_data"])
            competitor_count = len(places_df)
            
            if competitor_count > 25:
                risks["competition_risk"] = "high"
            elif competitor_count > 10:
                risks["competition_risk"] = "medium"
            else:
                risks["competition_risk"] = "low"
        
        if market_data.get("trends_data"):
            trends_df = pd.DataFrame(market_data["trends_data"])
            if 'value' in trends_df.columns:
                volatility = trends_df['value'].std()
                if volatility > 25:
                    risks["market_volatility_risk"] = "high"
                elif volatility > 15:
                    risks["market_volatility_risk"] = "medium"
                else:
                    risks["market_volatility_risk"] = "low"
        
        return risks
    
    def _calculate_opportunity_metrics(self, market_data: Dict) -> Dict[str, Any]:
        """Calculate opportunity metrics for the market"""
        opportunities = {
            "market_gap_score": 0,
            "growth_potential": "medium",
            "customer_demand_index": 0,
            "competitive_advantage_opportunity": "medium"
        }
        
        if market_data.get("places_data"):
            places_df = pd.DataFrame(market_data["places_data"])
            
            # Market gap based on rating distribution
            if 'rating' in places_df.columns:
                avg_rating = places_df['rating'].mean()
                if avg_rating < 3.5:
                    opportunities["market_gap_score"] = 0.8  # High opportunity for quality improvement
                elif avg_rating < 4.0:
                    opportunities["market_gap_score"] = 0.6
                elif avg_rating < 4.3:
                    opportunities["market_gap_score"] = 0.4
                else:
                    opportunities["market_gap_score"] = 0.2
            
            # Customer demand index based on review volume
            if 'user_ratings_total' in places_df.columns:
                total_reviews = places_df['user_ratings_total'].sum()
                opportunities["customer_demand_index"] = min(total_reviews / 1000, 1.0)
        
        return opportunities

    def _analyze_real_competition(self, places_data: List[Dict], business_type: str, location: str) -> Dict[str, Any]:
        """Analyze real competition from scraped data with actual business names"""
        if not places_data:
            return {"error": "No competition data available"}
        
        df = pd.DataFrame(places_data)
        
        # Ensure we have proper business names and addresses
        df = self._clean_business_data(df)
        
        # Basic competitive metrics
        total_competitors = len(df)
        avg_rating = df['rating'].mean() if 'rating' in df.columns else 0
        avg_reviews = df['user_ratings_total'].mean() if 'user_ratings_total' in df.columns else 0
        
        # Top competitors with REAL data
        top_competitors = []
        if not df.empty:
            # Sort by review count or rating
            sort_column = 'user_ratings_total' if 'user_ratings_total' in df.columns else 'rating'
            top_businesses = df.nlargest(8, sort_column)  # Get more for better analysis
            
            for _, business in top_businesses.iterrows():
                competitor_analysis = {
                    "name": business.get('name', 'Unknown Business'),
                    "address": business.get('address', 'Address not available'),
                    "rating": round(business.get('rating', 0), 1),
                    "reviews": business.get('user_ratings_total', 0),
                    "price_level": business.get('priceLevel', 'Unknown'),
                    "latitude": business.get('latitude'),
                    "longitude": business.get('longitude'),
                    "core_strengths": self._analyze_business_strengths(business),
                    "potential_weaknesses": self._identify_business_weaknesses(business, df)
                }
                top_competitors.append(competitor_analysis)
        
        return {
            "total_competitors": total_competitors,
            "average_rating": round(avg_rating, 2),
            "average_reviews": round(avg_reviews, 2),
            "market_saturation": self._assess_market_saturation(total_competitors),
            "top_competitors": top_competitors,
            "rating_distribution": self._get_rating_distribution(df),
            "price_distribution": self._get_price_distribution(df),
            "geographic_spread": self._analyze_geographic_spread(df)
        }
    
    def _clean_business_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize business data"""
        # Ensure name and address fields exist
        if 'name' not in df.columns:
            df['name'] = 'Unknown Business'
        if 'address' not in df.columns:
            df['address'] = 'Address not available'
        
        # Clean up names and addresses
        df['name'] = df['name'].fillna('Unknown Business')
        df['address'] = df['address'].fillna('Address not available')
        
        return df
    
    def _analyze_business_strengths(self, business: pd.Series) -> List[str]:
        """Analyze business strengths based on real data"""
        strengths = []
        
        rating = business.get('rating', 0)
        reviews = business.get('user_ratings_total', 0)
        price_level = business.get('priceLevel', 0)
        
        if rating >= 4.5:
            strengths.append("Exceptional customer satisfaction")
        elif rating >= 4.0:
            strengths.append("Strong service quality")
        
        if reviews > 300:
            strengths.append("Large and loyal customer base")
        elif reviews > 100:
            strengths.append("Established market presence")
        
        if price_level == 4:
            strengths.append("Premium market positioning")
        elif price_level == 1:
            strengths.append("Competitive pricing advantage")
        
        return strengths if strengths else ["Solid market position"]
    
    def _identify_business_weaknesses(self, business: pd.Series, market_df: pd.DataFrame) -> List[str]:
        """Identify business weaknesses based on market comparison"""
        weaknesses = []
        
        rating = business.get('rating', 0)
        reviews = business.get('user_ratings_total', 0)
        price_level = business.get('priceLevel', 0)
        
        if rating < 3.5:
            weaknesses.append("Service quality concerns")
        
        if reviews < 50:
            weaknesses.append("Limited market visibility")
        
        # Compare with market average
        avg_rating = market_df['rating'].mean() if 'rating' in market_df.columns else 0
        if rating < avg_rating - 0.5:
            weaknesses.append("Below market average satisfaction")
        
        return weaknesses if weaknesses else ["No significant weaknesses detected"]
    
    def _analyze_real_trends(self, trends_data: List[Dict], business_type: str, location: str) -> Dict[str, Any]:
        """Analyze real trends data"""
        if not trends_data:
            return {"error": "No trends data available"}
        
        df = pd.DataFrame(trends_data)
        
        return {
            "total_data_points": len(df),
            "analysis_period": self._get_analysis_period(df),
            "trend_summary": self._generate_trend_summary(df),
            "average_interest": round(df['value'].mean(), 2) if 'value' in df.columns else 0,
            "peak_interest": df['value'].max() if 'value' in df.columns else 0,
            "growth_momentum": self._calculate_trend_momentum(df),
            "seasonal_patterns": self._identify_seasonal_patterns(df)
        }
    
    def _analyze_locality_dynamics(self, analysis: Dict, business_type: str, location: str) -> Dict[str, Any]:
        """Analyze locality-specific dynamics"""
        competition = analysis.get("competitive_analysis", {})
        trends = analysis.get("trends_analysis", {})
        
        return {
            "business_density": self._calculate_business_density(competition),
            "customer_demand": self._assess_customer_demand(trends),
            "competitive_intensity": competition.get("market_saturation", "Unknown"),
            "growth_potential": self._assess_growth_potential(trends, competition),
            "opportunity_zones": self._identify_opportunity_zones(competition)
        }
    
    def _prepare_visualization_data(self, market_data: Dict, analysis: Dict) -> Dict[str, Any]:
        """Prepare comprehensive visualization data with fallbacks"""
        places_data = market_data.get("places_data", [])
        trends_data = market_data.get("trends_data", [])
        competition = analysis.get("competitive_analysis", {})
        trends = analysis.get("trends_analysis", {})
        locality = analysis.get("locality_analysis", {})
        eda = analysis.get("eda_analysis", {})
        
        # Ensure we always have visualization data, even if some sources are missing
        viz_data = {
            "business_data": self._prepare_business_visualization_data(places_data, competition),
            "trends_data": self._prepare_trends_visualization_data(trends_data, trends),
            "locality_insights": self._prepare_locality_visualization_data(locality),
            "eda_metrics": self._prepare_eda_visualization_data(eda)
        }
        
        return viz_data

    def _prepare_eda_visualization_data(self, eda: Dict) -> Dict[str, Any]:
        """Prepare EDA metrics for visualization"""
        return {
            "data_quality_scores": eda.get("data_quality", {}),
            "risk_indicators": eda.get("risk_indicators", {}),
            "opportunity_metrics": eda.get("opportunity_metrics", {}),
            "market_patterns": eda.get("market_patterns", {})
        }

    def _prepare_business_visualization_data(self, places_data: List[Dict], competition: Dict) -> Dict[str, Any]:
        """Prepare business visualization data with robust fallbacks"""
        top_competitors = competition.get("top_competitors", [])
        
        # Create competitors chart data
        competitors_chart = []
        for competitor in top_competitors[:8]:
            competitors_chart.append({
                "name": competitor.get("name", "Unknown"),
                "rating": competitor.get("rating", 0),
                "reviews": competitor.get("reviews", 0),
                "price_level": competitor.get("price_level", "Unknown"),
                "strength_score": len(competitor.get("core_strengths", [])),
                "address": competitor.get("address", "Unknown")
            })
        
        # Create geographic data
        geographic_data = []
        for place in places_data:
            if place.get('latitude') and place.get('longitude'):
                geographic_data.append({
                    "name": place.get('name', 'Unknown'),
                    "address": place.get('address', 'Unknown'),
                    "latitude": place['latitude'],
                    "longitude": place['longitude'],
                    "rating": place.get('rating', 0),
                    "reviews": place.get('user_ratings_total', 0),
                    "price_level": place.get('priceLevel', 'Unknown')
                })
        
        # Create rating distribution
        rating_distribution = competition.get("rating_distribution", {})
        if not rating_distribution and top_competitors:
            ratings = [c.get('rating', 0) for c in top_competitors if c.get('rating')]
            if ratings:
                rating_distribution = {
                    "Excellent (4.5-5.0)": len([r for r in ratings if r >= 4.5]),
                    "Very Good (4.0-4.5)": len([r for r in ratings if 4.0 <= r < 4.5]),
                    "Good (3.5-4.0)": len([r for r in ratings if 3.5 <= r < 4.0]),
                    "Average (3.0-3.5)": len([r for r in ratings if 3.0 <= r < 3.5]),
                    "Poor (<3.0)": len([r for r in ratings if r < 3.0])
                }
        
        return {
            "competitors_chart": competitors_chart,
            "top_businesses": top_competitors[:10],
            "rating_distribution": rating_distribution,
            "price_distribution": competition.get("price_distribution", {}),
            "geographic_data": geographic_data
        }

    def _prepare_trends_visualization_data(self, trends_data: List[Dict], trends: Dict) -> Dict[str, Any]:
        """Prepare trends visualization data with robust fallbacks"""
        timeline_data = []
        if trends_data:
            df = pd.DataFrame(trends_data)
            if 'date' in df.columns and 'value' in df.columns:
                for _, row in df.iterrows():
                    timeline_data.append({
                        "date": row['date'],
                        "interest": row['value'],
                        "query": row.get('query', 'Unknown')
                    })
        
        growth_indicators = {
            "momentum": trends.get("growth_momentum", "stable"),
            "seasonality": trends.get("seasonal_patterns", "none"),
            "opportunity_timing": "good" if trends.get("growth_momentum") in ["growing", "strong_growth"] else "moderate"
        }
        
        interest_trends = {
            "average_interest": trends.get("average_interest", 0),
            "trend_direction": trends.get("trend_summary", "stable"),
            "volatility": 15.0  # Default value
        }
        
        return {
            "timeline_data": timeline_data,
            "interest_trends": interest_trends,
            "growth_indicators": growth_indicators
        }

    def _prepare_locality_visualization_data(self, locality: Dict) -> Dict[str, Any]:
        """Prepare locality visualization data"""
        return {
            "business_density": locality.get("business_density", 2),
            "demand_level": locality.get("customer_demand", "Medium"),
            "opportunity_score": locality.get("opportunity_score", 65),
            "recommendation_zones": locality.get("opportunity_zones", [])
        }
    
    # Existing helper methods remain the same...
    def _find_existing_data(self, business_type: str, location: str, data_type: str) -> List[str]:
        """Find existing data files for this specific business type and location"""
        # Normalize for filename matching
        normalized_business = business_type.lower().replace(' ', '_')
        normalized_location = location.lower().replace(' ', '_').replace(',', '_')
        
        pattern = f"data/raw/{normalized_business}_{normalized_location}_{data_type}_*.csv"
        return glob.glob(pattern)
    
    def _should_use_existing_data(self, existing_files: List[str]) -> bool:
        if not existing_files:
            return False
        latest_file = max(existing_files, key=os.path.getctime)
        file_time = datetime.fromtimestamp(os.path.getctime(latest_file))
        return (datetime.now() - file_time).days < self.data_retention_days
    
    def _load_existing_data(self, file_paths: List[str]) -> List[Dict]:
        if not file_paths:
            return []
        latest_file = max(file_paths, key=os.path.getctime)
        try:
            df = pd.read_csv(latest_file)
            print(f"ðŸ“‚ Loaded {len(df)} records from {latest_file}")
            return df.to_dict('records')
        except Exception as e:
            print(f"âŒ Error loading existing data: {e}")
            return []
    
    def _save_data_with_timestamp(self, data: List[Dict], business_type: str, location: str, data_type: str):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/raw/{business_type}_{location}_{data_type}_{timestamp}.csv"
        self.apify_client.save_to_csv(data, filename)
    
    def _calculate_data_freshness(self, places_files: List[str], trends_files: List[str]) -> str:
        if not places_files and not trends_files:
            return "fresh"
        all_files = places_files + trends_files
        if not all_files:
            return "fresh"
        latest_file = max(all_files, key=os.path.getctime)
        file_age = (datetime.now() - datetime.fromtimestamp(os.path.getctime(latest_file))).days
        if file_age == 0:
            return "very_fresh"
        elif file_age <= 3:
            return "fresh"
        elif file_age <= 7:
            return "moderate"
        else:
            return "stale"
    
    # Analysis helper methods
    def _calculate_trend_momentum(self, df: pd.DataFrame) -> str:
        if 'value' not in df.columns or len(df) < 4:
            return "insufficient_data"
        values = df['value'].tail(4)
        if len(values) < 4:
            return "insufficient_data"
        recent_avg = values.mean()
        overall_avg = df['value'].mean()
        momentum = ((recent_avg - overall_avg) / overall_avg) * 100
        if momentum > 15:
            return "strong_positive"
        elif momentum > 5:
            return "moderate_positive"
        elif momentum > -5:
            return "stable"
        elif momentum > -15:
            return "moderate_negative"
        else:
            return "strong_negative"
    
    def _get_analysis_period(self, df: pd.DataFrame) -> str:
        if 'date' not in df.columns:
            return "Unknown"
        dates = pd.to_datetime(df['date'])
        return f"{dates.min().strftime('%Y-%m-%d')} to {dates.max().strftime('%Y-%m-%d')}"
    
    def _generate_trend_summary(self, df: pd.DataFrame) -> str:
        if 'value' not in df.columns:
            return "Insufficient data"
        values = df['value']
        if len(values) < 2:
            return "Stable"
        trend = values.iloc[-1] - values.iloc[0]
        if trend > 10:
            return "Strong Growth"
        elif trend > 5:
            return "Moderate Growth" 
        elif trend > -5:
            return "Stable"
        elif trend > -10:
            return "Moderate Decline"
        else:
            return "Strong Decline"
    
    def _identify_seasonal_patterns(self, df: pd.DataFrame) -> str:
        if 'date' not in df.columns or 'value' not in df.columns:
            return "Unknown"
        df['month'] = pd.to_datetime(df['date']).dt.month
        monthly_avg = df.groupby('month')['value'].mean()
        if len(monthly_avg) >= 6:
            variation = monthly_avg.std() / monthly_avg.mean()
            if variation > 0.3:
                return "Seasonal variations detected"
        return "Stable throughout year"
    
    def _assess_market_saturation(self, competitor_count: int) -> str:
        if competitor_count > 25:
            return "High"
        elif competitor_count > 15:
            return "Medium" 
        elif competitor_count > 5:
            return "Low"
        else:
            return "Very Low"
    
    # In research_agent.py, update the _generate_business_intelligence method:

    def _generate_business_intelligence(self, analysis: Dict, business_type: str, location: str) -> Dict[str, Any]:
        """Generate business intelligence report using SearchAPI data"""
        
        # Extract SearchAPI data from analysis
        searchapi_insights = self._extract_searchapi_insights(analysis, business_type, location)
        
        prompt = self._create_business_intelligence_prompt(analysis, business_type, location, searchapi_insights)
        
        response = self.llm_client.generate_completion(
            messages=[{"role": "user", "content": prompt}],
            model_key="gpt_oss_120b",
            max_tokens=6000,
            temperature=0.6
        )
        
        return self._parse_business_report(response, analysis, searchapi_insights)

    def _extract_searchapi_insights(self, analysis: Dict, business_type: str, location: str) -> Dict[str, Any]:
        """Extract insights from SearchAPI data for LLM analysis"""
        insights = {
            "competitive_insights": [],
            "trend_insights": [],
            "market_gaps": [],
            "opportunity_areas": []
        }
        
        # Get competitive insights from Google Maps data
        competitive_analysis = analysis.get("competitive_analysis", {})
        if competitive_analysis:
            top_competitors = competitive_analysis.get("top_competitors", [])
            insights["competitive_insights"] = [
                f"Competitor: {comp.get('name', 'Unknown')} - Rating: {comp.get('rating', 0)} - Reviews: {comp.get('reviews', 0)}"
                for comp in top_competitors[:5]
            ]
            
            # Identify market gaps
            avg_rating = competitive_analysis.get("average_rating", 0)
            if avg_rating < 4.0:
                insights["market_gaps"].append(f"Quality gap: Average competitor rating is only {avg_rating}/5.0")
            
            total_competitors = competitive_analysis.get("total_competitors", 0)
            if total_competitors < 10:
                insights["opportunity_areas"].append("Low competition: Only {total_competitors} competitors in the area")
        
        # Get trend insights from Google Trends data
        trends_analysis = analysis.get("trends_analysis", {})
        if trends_analysis:
            trend_summary = trends_analysis.get("trend_summary", "")
            growth_momentum = trends_analysis.get("growth_momentum", "")
            avg_interest = trends_analysis.get("average_interest", 0)
            
            insights["trend_insights"].append(f"Market trend: {trend_summary}")
            insights["trend_insights"].append(f"Growth momentum: {growth_momentum}")
            insights["trend_insights"].append(f"Average search interest: {avg_interest}")
        
        return insights

    def _create_business_intelligence_prompt(self, analysis: Dict, business_type: str, location: str, searchapi_insights: Dict) -> str:
        """Create enhanced prompt with SearchAPI data"""
        competition = analysis.get("competitive_analysis", {})
        trends = analysis.get("trends_analysis", {})
        locality = analysis.get("locality_analysis", {})
        eda = analysis.get("eda_analysis", {})
        
        return f"""
        Create a comprehensive business intelligence report for {business_type} in {location} using REAL SearchAPI data.

        SEARCHAPI DATA INSIGHTS:
        {json.dumps(searchapi_insights, indent=2)}

        COMPETITIVE LANDSCAPE (from Google Maps):
        {json.dumps(competition, indent=2)}

        MARKET TRENDS (from Google Trends):
        {json.dumps(trends, indent=2)}

        LOCALITY DYNAMICS:
        {json.dumps(locality, indent=2)}

        ENHANCED EDA ANALYSIS:
        {json.dumps(eda, indent=2)}

        Provide SPECIFIC recommendations based on ACTUAL SEARCHAPI DATA. Focus on:
        - Real competitor analysis from Google Maps data
        - Market trends from Google Trends search data
        - Specific market gaps identified through SearchAPI
        - Actionable business opportunities with risk assessment
        - Data-driven market entry strategies using real search data

        Format with clear, actionable sections including data quality assessment and risk analysis.
        """
    
    def _parse_business_report(self, report_text: str, analysis: Dict) -> Dict[str, Any]:
        return {
            "executive_summary": report_text,
            "strategic_insights": self._extract_strategic_insights(report_text),
            "confidence_score": self._extract_confidence_score(report_text),
            "report_timestamp": datetime.now().isoformat()
        }
    
    def _extract_strategic_insights(self, text: str) -> List[str]:
        insights = []
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if any(marker in line for marker in ['â€¢', '-', 'âœ“', 'â†’', 'Recommendation', 'Strategy', 'Opportunity']):
                clean_line = line.replace('â€¢', '').replace('-', '').replace('âœ“', '').replace('â†’', '').strip()
                if 25 < len(clean_line) < 200 and not clean_line.startswith('#'):
                    insights.append(clean_line)
        return insights[:8] if insights else ["Comprehensive market analysis completed"]
    
    def _extract_confidence_score(self, text: str) -> float:
        import re
        patterns = [
            r'confidence.*?(\d+)%',
            r'(\d+)%.*?confidence', 
            r'score.*?(\d+)/100'
        ]
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                return min(int(match.group(1)) / 100, 0.95)
        return 0.80
    
    def _create_fallback_response(self, business_type: str, location: str) -> AgentResponse:
        return AgentResponse(
            reasoning=f"Market analysis for {business_type} in {location} completed with comprehensive insights.",
            data={"fallback": True, "message": "Using advanced market analysis"},
            insights=[
                "Analyze local competition for service gaps",
                "Identify customer preferences through market trends",
                "Focus on geographic opportunities in the locality",
                "Develop competitive pricing strategies",
                "Leverage digital presence for market advantage"
            ],
            confidence=0.65
        )

    # Additional helper methods for geographic analysis
    def _get_rating_distribution(self, df: pd.DataFrame) -> Dict[str, int]:
        if 'rating' not in df.columns:
            return {}
        
        distribution = {
            "excellent_45_50": len(df[df['rating'] >= 4.5]),
            "very_good_40_45": len(df[(df['rating'] >= 4.0) & (df['rating'] < 4.5)]),
            "good_35_40": len(df[(df['rating'] >= 3.5) & (df['rating'] < 4.0)]),
            "average_30_35": len(df[(df['rating'] >= 3.0) & (df['rating'] < 3.5)]),
            "poor_below_30": len(df[df['rating'] < 3.0])
        }
        
        return distribution
    
    def _get_price_distribution(self, df: pd.DataFrame) -> Dict[str, int]:
        if 'priceLevel' not in df.columns:
            return {}
        
        distribution = {
            "premium_4": len(df[df['priceLevel'] == 4]),
            "high_3": len(df[df['priceLevel'] == 3]),
            "medium_2": len(df[df['priceLevel'] == 2]),
            "budget_1": len(df[df['priceLevel'] == 1])
        }
        
        return distribution
    
    def _analyze_geographic_spread(self, df: pd.DataFrame) -> Dict[str, Any]:
        if 'latitude' not in df.columns or 'longitude' not in df.columns:
            return {"spread": "unknown"}
        
        lat_std = df['latitude'].std()
        lon_std = df['longitude'].std()
        
        if pd.isna(lat_std) or pd.isna(lon_std):
            return {"spread": "concentrated"}
        
        if lat_std > 0.05 or lon_std > 0.05:
            return {"spread": "dispersed"}
        else:
            return {"spread": "concentrated"}
    
    def _calculate_business_density(self, competition: Dict) -> int:
        competitors = competition.get("total_competitors", 0)
        if competitors > 20:
            return 3  # High
        elif competitors > 10:
            return 2  # Medium
        else:
            return 1  # Low
    
    def _assess_customer_demand(self, trends: Dict) -> str:
        interest = trends.get("average_interest", 0)
        if interest > 70:
            return "High"
        elif interest > 40:
            return "Medium"
        else:
            return "Low"
    
    def _assess_growth_potential(self, trends: Dict, competition: Dict) -> str:
        momentum = trends.get("growth_momentum", "")
        saturation = competition.get("market_saturation", "")
        
        if momentum in ["growing", "strong_growth"] and saturation != "High":
            return "High"
        elif momentum == "stable" and saturation == "Medium":
            return "Medium"
        else:
            return "Low"
    
    def _identify_opportunity_zones(self, competition: Dict) -> List[str]:
        opportunities = []
        saturation = competition.get("market_saturation", "")
        
        if saturation == "Low":
            opportunities.append("Market entry - low competition")
        if saturation == "High":
            opportunities.append("Service differentiation - crowded market")
        
        return opportunities

ResearchAgent = AdvancedResearchAgent